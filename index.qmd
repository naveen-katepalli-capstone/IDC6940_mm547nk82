---
title: "Clustering Traffic Accidents Using K-Means Clustering"
subtitle: "This is a Report Template Quarto"
author: "Naveen Katepalli,
        Mrinal Mandapaka,
        Adiba Khan"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

::: callout-important
**Remember:** Your goal is to make your audience understand and care
about your findings. By crafting a compelling story, you can effectively
communicate the value of your data science project.

Carefully read this template since it has instructions and tips to
writing!
:::

## Introduction


## Project Overview

This capstone project applies K-Means Clustering to analyze traffic accident data, aiming to identify high-risk areas ("hotspots") and uncover contributing factors such as location, time, weather, or road conditions. By grouping similar accidents into clusters, the project provides actionable insights for transportation authorities to implement targeted safety measures, reducing accident frequency and severity (Road accident prediction and model interpretation using a hybrid K-means and random forest algorithm approach, Discover Applied Sciences (2020)).

## K-Means Clustering for Traffic Accident Analysis

K-Means Clustering, an unsupervised learning algorithm, segments traffic accident data into K clusters, ensuring incidents within a cluster share similar characteristics while differing from other clusters (Unsupervised K-Means Clustering Algorithm, Sinaga & Yang, IEEE Access (2020); K-Means Clustering Algorithm: A Brief Review, Bao Chong, Francis Academic Press (2021)). The process involves:

-   Centroid Initialization: 
Select K initial centroids, ideally using principal component analysis (PCA) to enhance convergence (Efficient Feature Selection with Hybrid K-Means Genetic Algorithm for Text Clustering, Zubair et al., 2022 4th International Conference on Electrical, Computer & Telecommunication Engineering (ICECTE) (2022)).

-   Data Assignment: 
Assign each accident record to the nearest centroid based on Euclidean distance, using features like geographic coordinates, time, or weather (Implementation of K-Means Clustering for Scheduling Courses of Lecturers, Fahlevi et al., IOP Conference Series: Materials Science and Engineering (2020)).

-   Centroid Update: 
Recalculate centroids as the mean of assigned points (Infected Fruit Part Detection Using K-Means Clustering Segmentation Technique, Dubey et al., International Journal of Interactive Multimedia and Artificial Intelligence (2013)).

-   Iteration: 
Repeat until clusters stabilize (Classification of Aquifer Vulnerability Using K-Means Cluster Analysis, Javadi et al., Journal of Hydrology (2017)).
This project uses datasets like New York City’s construction-related accidents or Indian traffic data to cluster incidents by spatial, temporal, or environmental factors, identifying high-risk zones (A data mining framework to analyze road accident data, Journal of Big Data (2015); workzone-collision-analysis/capstone, GitHub (2020)).

  Determining the Optimal Number of Clusters
  Selecting the optimal K ensures meaningful accident clusters. Methods include:

-   Elbow Method: Plots sum of squared errors (SSE) against K, with the "elbow" indicating the optimal number of clusters (Integration K-Means Clustering Method and Elbow Method for Identification of The Best Customer Profile Cluster, Syakur et al., IOP Conference Series: Materials Science and Engineering (2018)).

-   Silhouette Method: Evaluates cluster quality using silhouette coefficients (-1 to 1), where 1 indicates well-separated clusters, 0 suggests overlap, and -1 implies misassignment (Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis, Rousseeuw, Journal of Computational and Applied Mathematics (1987); A Quantitative Discriminant Method of Elbow Point for the Optimal Number of Clusters in Clustering Algorithm, Shi et al., EURASIP Journal on Wireless Communications and Networking (2021)). Weighted variants improve accuracy (Silhouette-Coefficient Based Weighting K-Means, Lai et al., Neural Computing and Applications (2024); Silhouette-Guided Instance-Weighted K-Means (K-Sil), Semoglou, Likas & Pavlopoulos, arXiv (2025)).

-   Gap Statistic: Compares within-cluster dispersion to a reference distribution (Estimating the Number of Clusters in a Data Set via the Gap Statistic, Tibshirani, Walther & Hastie, Journal of the Royal Statistical Society: Series B (Statistical Methodology) (2001)).
The project employs the Elbow and Silhouette methods to determine K, ensuring robust hotspot identification (K-Means Clustering, Wikipedia (2025); Silhouette (Clustering), Wikipedia (2025)).

### Optimization and Preprocessing

Large accident datasets benefit from the Canopy algorithm, which partitions data into subsets for efficient clustering (Research on K-Value Selection Method of K-Means Clustering Algorithm, Yuan & Yang, Journal of Multidisciplinary Scientific Journal (2019); Clustering Analysis of Traffic Accident Dataset using Canopy K Means, IEEE Conference Publication (2020)). PCA reduces dimensionality, focusing on key features like location or road conditions (Efficient Feature Selection with Hybrid K-Means Genetic Algorithm for Text Clustering, Zubair et al., 2022 4th International Conference on Electrical, Computer & Telecommunication Engineering (ICECTE) (2022)). Silhouette-weighted K-Means enhances cluster quality by prioritizing well-separated incidents (Silhouette-Coefficient Based Weighting K-Means, Lai et al., Neural Computing and Applications (2024)).

### Cluster Quality Evaluation

The Calinski-Harabasz index validates cluster quality, with higher values indicating well-defined accident clusters (The Clustering Quality Based on Calinski-Harabasz Index, Qabbaah, Sammour & Vanhoof, European Journal of Scientific Research (2019)).

### Application to Traffic Accident Hotspots

The project clusters traffic accident data to identify hotspots and contributing factors. For example, a London study used K-Means with kernel density estimation to group accidents into five categories and 15 clusters based on environmental factors, guiding safety campaigns (Kernel density estimation and K-means clustering to profile road accident hotspots, ScienceDirect (2007)). In India, K-Modes Clustering on 11,574 accidents in Dehradun revealed patterns when combined with association rule mining (A data mining framework to analyze road accident data, Journal of Big Data (2015)). Similarly, in Medellín, Colombia, K-Means clustered zones by venue composition to analyze collision trends (Modelling road traffic collisions using clustered zones based on Foursquare data in Medellín, ScienceDirect (2020)). This project analyzes datasets like New York City’s 20,000+ construction-related accidents to identify high-risk road segments, potentially due to poor lighting or road geometry, as seen in Iranian studies (Identifying accident prone areas and factors influencing the severity of crashes using machine learning and spatial analyses, Scientific Reports (2024); workzone-collision-analysis/capstone, GitHub (2020)). Results can guide interventions like improved signage or traffic management.

## Variations of K-Means

Variants enhance K-Means for accident data:

-   Constrained K-Means: Ensures meaningful cluster sizes for hotspot analysis (Constrained K-Means Clustering and Its Application to Pattern Recognition, Usami, The Japanese Journal of Behaviormetrics (2014)).

-   Improved K-Means: Reduces computational overhead for large datasets (Research on K-Means Clustering Algorithm: An Improved K-Means Clustering Algorithm, Na, Xumin & Yong, 2010 Third International Symposium on Intelligent Information Technology and Security Informatics (2010)).

-   Filtering Algorithm: Uses kd-trees for efficient centroid selection (An Efficient K-Means Clustering Algorithm: Analysis and Implementation, Kanungo et al., IEEE Transactions on Pattern Analysis and Machine Intelligence (2002)).

-   Genetic-Based K-Means (GBKM): Optimizes centroids via genetic algorithms (Genetic K-Means Clustering Algorithm for Documents, Chougule et al., International Journal of Computer Applications (2015)).
MapReduce K-Means: Processes large-scale accident data (MapReduce Design of K-Means Clustering Algorithm, Anchalia, Koundinya & Srinath, 2013 International Conference on Information Science and Applications (ICISA) (2013)).

-   Silhouette-Weighted K-Means: Improves hotspot detection by weighting incidents (Silhouette-Guided Instance-Weighted K-Means (K-Sil), Semoglou, Likas & Pavlopoulos, arXiv (2025)).
Limitations

K-Means struggles with random centroid initialization, which may lead to inconsistent hotspots. Predefining K risks variability, and the algorithm falters with complex, heterogeneous accident data. Careful preprocessing, such as handling missing values and standardizing features, is crucial (The K-Means Algorithm: A Comprehensive Survey and Performance Evaluation, Ahmed, Seraj & Islam, Electronics (2020); K-Means Clustering, Wikipedia (2025)).

### Project Impact

By identifying accident hotspots and patterns, this project supports transportation authorities in prioritizing interventions, such as enhanced lighting or road redesign, to improve safety (Road accident prediction and model interpretation using a hybrid K-means and random forest algorithm approach, Discover Applied Sciences (2020)). The findings align with studies like those in London and Medellín, offering data-driven solutions to reduce traffic accidents (Kernel density estimation and K-means clustering to profile road accident hotspots, ScienceDirect (2007); Modelling road traffic collisions using clustered zones based on Foursquare data in Medellín, ScienceDirect (2020)).

## Methods
The approach to K-Means Clustering for this capstone project involves preparing the data, selecting relevant variables, determining the ideal number of clusters, executing the clustering process, and analyzing the resulting groups to identify traffic accident patterns.

### Data Preparation

Data preparation begins by removing categorical variables (e.g., vehicle type), eliminating outliers (e.g., incorrect coordinates), and applying feature scaling (e.g., normalizing location or time data) to ensure consistency (Raykov et al. 2016).

### Variable Selection

Key variables are chosen, such as accident coordinates, time, weather, and severity, to highlight factors influencing traffic accident hotspots (A data mining framework to analyze road accident data, Journal of Big Data (2015)).

### Optimal Cluster Determination

The optimal number of clusters is assessed using three techniques:

-   Silhouette Coefficient: Evaluates how well each accident fits its cluster compared to others, with scores from -1 to 1 (1 indicating a good fit). It is calculated as (Shi et al. 2021):
$$S = \frac{b - a}{\max(a, b)}$$
where a is the average intra-cluster distance, and b is the minimum average distance to other clusters.

-   Elbow Method: Plots within-cluster sum of squares (WCSS) against cluster numbers, selecting the point where the decrease rate slows. WCSS is defined as (Shi et al. 2021): 
$$WCSS = \sum_{k=1}^{K} \sum_{x \in C_k} ||x - \mu_k||^2$$
where x is a data point, $C_k$ is the cluster, $μ_k$ is the centroid, and K is the total clusters.

-   Gap Statistic: Compares within-cluster dispersion to a null reference to find the best K (Tibshirani, Walther & Hastie 2001):
 $$Gap_n(k) = E_n^*\{\log(W_k)\} - \log(W_k)$$
 

 where $E_n$ is the expected dispersion, n is sample size, and $W_k$ is within-cluster sum of squares.

### Clustering Process
K-Means Clustering is an iterative procedure (Fahlevi et al. 2020):

-   Set the number of clusters.

-   Choose initial centroids randomly.

-   Assign data points to the nearest centroid using Euclidean distance: 
$$d(x, y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}$$

where x and y are points (Ultsch & Lötsch 2022).

-   Recalculate centroids as the mean of assigned points.

-   Repeat until centroids stabilize.




## Analysis and Results

### Data Exploration and Visualization
A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(ggplot2)
library(dplyr)
library(cluster)
library(factoextra)
library(tidyr)
library(psych)
library(stats)
library(corrplot)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data

df <- read.csv("Fatalities.csv")  # Use your actual CSV file path

Fatalities_Grouped_By_State <- df %>%
  group_by(state, year) %>%
  summarise(
    afatal = sum(afatal, na.rm = TRUE),
    fatal1517 = sum(fatal1517, na.rm = TRUE),
    fatal1820 = sum(fatal1820, na.rm = TRUE),
    .groups = "drop"
  )

print(Fatalities_Grouped_By_State)
print(summary(Fatalities_Grouped_By_State))
glimpse(Fatalities_Grouped_By_State)

```

```{r, warning=FALSE, echo=TRUE}
hist_plot <- ggplot(Fatalities_Grouped_By_State, aes(x = afatal)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "black") +
  labs(title = "Distribution of Alcohol-Related Fatalities",
       x = "Alcohol Fatalities", y = "Frequency")
print(hist_plot)
```

### Interpretation:

-   Most states have alcohol fatalities clustered in lower ranges (under ~400).

-   A few extreme values (e.g., >1000) suggest outliers—likely populous states like California or Texas.

-   Distribution is right-skewed, meaning a few states experience disproportionately high fatalities.

```{r, warning=FALSE, echo=TRUE}
scatter_1517 <- ggplot(Fatalities_Grouped_By_State, aes(x = afatal, y = fatal1517)) +
  geom_point(color = "darkred") +
  labs(title = "Alcohol Fatalities vs Fatalities Ages 15-17",
       x = "Alcohol Fatalities", y = "Fatalities Ages 15-17")
print(scatter_1517)
```

### Interpretation:

-   A positive trend is visible: states with higher afatal tend to have more teen fatalities.

-   However, there is scatter and noise, suggesting other factors also influence teen fatalities (e.g., policy, enforcement).

```{r, warning=FALSE, echo=TRUE}
scatter_1820 <- ggplot(Fatalities_Grouped_By_State, aes(x = afatal, y = fatal1820)) +
  geom_point(color = "darkgreen") +
  labs(title = "Alcohol Fatalities vs Fatalities Ages 18-20",
       x = "Alcohol Fatalities", y = "Fatalities Ages 18-20")
print(scatter_1820)

```
### Interpretation:
-   Shows a stronger linear relationship than with ages 15–17.

-   Suggests this age group contributes more significantly to overall alcohol fatalities.

-   Could point to risky behavior or access to alcohol in college-age populations.


```{r correlation-plot, echo=TRUE, warning=FALSE, message=FALSE}
# Load required libraries
library(dplyr)
library(corrplot)

# Select numeric columns
numeric_cols <- Fatalities_Grouped_By_State %>%
  select(afatal, fatal1517, fatal1820)

# Compute correlation matrix
cor_matrix <- cor(numeric_cols, use = "complete.obs")

# Plot the correlation matrix
corrplot(cor_matrix, method = "number", type = "upper")

```

### Interpretation:
-   afatal vs fatal1820 = 0.8 → strong positive correlation

-   afatal vs fatal1517 = 0.6 → moderate correlation

-   fatal1517 vs fatal1820 = 0.7 → teens in both groups follow similar trends


## Modeling and Results

This section details the data preprocessing, modeling approach, and key findings, weaving a narrative about how the data reveals patterns in traffic accident fatalities across U.S. states from 1982–1988. The story focuses on identifying high-risk areas and understanding the role of alcohol and young drivers in fatal crashes.

### Data Preprocessing and Cleaning
To ensure robust clustering, the "Fatalities.csv" dataset was meticulously preprocessed:

-   Variable Selection: Focused on afatal (alcohol-related fatalities), fatal1517 (fatalities ages 15–17), and fatal1820 (fatalities ages 18–20) to capture alcohol-related and teen-related accident patterns, critical for identifying high-risk hotspots.
-   Missing Data Check: No missing or null values were found, ensuring data integrity.

-   Outlier Removal: Outliers were identified using the interquartile range (IQR) method, removing values beyond ( Q1 - 1.5 \times IQR ) or ( Q3 + 1.5 \times IQR ) to prevent skewing cluster centroids.

-   Feature Scaling: Standardized afatal, fatal1517, and fatal1820 to a mean of 0 and standard deviation of 1, as K-Means clustering relies on Euclidean distance, and unscaled data could bias results.

**Tell a story about what the data reveals.**

```{r}
# Define a function to detect outliers using the IQR method
detect_outliers <- function(x) {
  q1 <- quantile(x, probs = 0.25, na.rm = TRUE)
  q3 <- quantile(x, probs = 0.75, na.rm = TRUE)
  iqr <- q3 - q1
  x < (q1 - 1.5 * iqr) | x > (q3 + 1.5 * iqr)
}

# Define a function to eliminate rows with outliers from specified columns
eliminate_outliers <- function(df, cols = names(df)) {
  for (col in cols) {
    df <- df[!detect_outliers(df[[col]]), ]
  }
  return(df)
}

# Specify the columns to check for outliers
target_columns <- c("afatal", "fatal1517", "fatal1820")

# Remove rows with outliers in the target columns
cleaned_data <- eliminate_outliers(Fatalities_Grouped_By_State, target_columns)

# Scale the cleaned data for clustering or further analysis
scaled_data <- scale(cleaned_data[, target_columns])

# Visualizing Scaled Data
boxplot(
  scaled_data,
  ylab = "Scaled Values",
  main = "Boxplot of Scaled Variables (After Outlier Removal)"
)

```


```{r}
# Determining Optimal Number of Clusters: Elbow Method
library(factoextra)  # For visualizing clustering results

# Apply the Elbow Method using within-cluster sum of squares (WSS)
fviz_nbclust(
  scaled_data,             # The scaled data prepared for clustering
  FUNcluster = kmeans,     # Clustering method: k-means
  method = "wss"           # WSS: within-cluster sum of squares
) +
  labs(
    title = "Elbow Method for Determining Optimal Clusters",
    x = "Number of Clusters (k)",
    y = "Total Within-Cluster Sum of Squares"
  )

```

```{r}
# K-Means Clustering of Traffic Fatalities
set.seed(123)

# Perform K-Means clustering with 3 clusters and 25 random starts
kmeans_result <- kmeans(
  scaled_data,      # Scaled data
  centers = 3,      # Number of clusters
  nstart = 25       # Number of random initial configurations
)

# Add the cluster assignments to the cleaned dataset
cleaned_data$cluster <- as.factor(kmeans_result$cluster)

# Visualizing the Clusters
library(factoextra)

# Visualize the clusters using fviz_cluster
fviz_cluster(
  object = kmeans_result,          # K-means result object
  data = scaled_data,              # Scaled data used for clustering
  ellipse.type = "convex",         # Convex hull around clusters
  geom = "point",                  # Points for each observation
  palette = "jco",                 # Color palette
  ggtheme = theme_minimal()        # Minimal ggplot theme
) +
  labs(
    title = "K-Means Clustering of Traffic Fatalities",
    x = "Principal Component 1",
    y = "Principal Component 2"
  )

```

```{r}
library(dplyr)
library(knitr)
library(kableExtra)

# Create cluster summary
cluster_summary <- cleaned_data %>%
  group_by(cluster) %>%
  summarise(
    across(where(is.numeric), ~ mean(.x, na.rm = TRUE)),
    state = NA_character_,
    .groups = "drop"
  )

# Display the summary nicely
cluster_summary %>%
  kable(caption = "Table: Average Values of Numeric Variables by Cluster", digits = 2, booktabs = TRUE) %>%
  kable_styling(full_width = FALSE, position = "center")

```

```{r}
# ---------------------------------------------------------
# Bar Plot: Alcohol-Related Fatalities by Cluster
# ---------------------------------------------------------

# Load required library
library(ggplot2)

# Create bar plot for average alcohol-related fatalities per cluster
ggplot(cluster_summary, aes(x = cluster, y = afatal, fill = cluster)) +
  geom_bar(
    stat = "identity",
    show.legend = FALSE  # Hides redundant legend since color = cluster
  ) +
  scale_fill_manual(values = c("red", "green", "blue")) +
  labs(
    title = "Alcohol-Related Fatalities by Cluster",
    x = "Cluster",
    y = "Average Alcohol-Related Fatalities"
  ) +
  theme_minimal()

```

```{r}
# Principal Component Analysis (PCA)
# Perform PCA on the scaled data
pca_result <- prcomp(
  scaled_data,      # Data already scaled for clustering
  center = TRUE,    # Center the variables
  scale. = TRUE     # Scale the variables to unit variance
)

# Scree Plot: Variance Explained by Principal Components
library(factoextra)

# Visualize the proportion of variance explained by each principal component
fviz_eig(pca_result) +
  labs(
    title = "Scree Plot of PCA Variance Explained",
    x = "Principal Components",
    y = "Percentage of Variance Explained"
  ) +
  theme_minimal()
```


```{r}
# PCA Variable Correlation Circle
# Visualize how original variables contribute to the principal components
fviz_pca_var(
  pca_result,                       # PCA result object
  col.var = "contrib",             # Color by contribution to PCs
  gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),  # Gradient for contribution
  repel = TRUE                     # Avoid label overlap
) +
  labs(
    title = "PCA Variable Correlation Circle",
    x = "Principal Component 1",
    y = "Principal Component 2"
  ) +
  theme_minimal()
```

```{r}

# PCA Projection of Observations by Cluster
# Visualize observations in the PCA space, colored by cluster
fviz_pca_ind(
  pca_result,                   # PCA result object
  geom.ind = "point",           # Use points to represent observations
  col.ind = cleaned_data$cluster,  # Color points by cluster assignment
  palette = "jco",              # Use a clean and readable color palette
  addEllipses = TRUE,           # Draw ellipses around clusters
  legend.title = "Cluster"      # Title for legend
) +
  labs(
    title = "PCA Projection of Observations by Cluster",
    x = "Principal Component 1",
    y = "Principal Component 2"
  ) +
  theme_minimal()
```


```{r}
# ---------------------------------------------------------
# Geographic Distribution of Clusters
# ---------------------------------------------------------

# Create a lookup table to map state abbreviations to full state names (lowercase)
state_lookup <- data.frame(
  abbr = tolower(state.abb),
  full = tolower(state.name)
)

# Add special handling for Washington, D.C., which isn't included in base R state vectors
state_lookup <- rbind(
  state_lookup,
  data.frame(abbr = "dc", full = "district of columbia")
)

# Convert state abbreviations in the dataset to full state names
cleaned_data$state <- state_lookup$full[match(tolower(cleaned_data$state), state_lookup$abbr)]

# Prepare Data for Mapping -  Get U.S. map data from ggplot2
us_map <- map_data("state")

# Assign cluster values to map data based on matching state names
us_map$cluster <- cleaned_data$cluster[match(us_map$region, cleaned_data$state)]
us_map$cluster <- as.factor(us_map$cluster)

# Plotting the U.S. Map Colored by Cluster
library(ggplot2)
library(maps)

# Plot the map with states filled by cluster color
ggplot(us_map, aes(x = long, y = lat, group = group, fill = cluster)) +
  geom_polygon(color = "black", linewidth = 0.25) +
  theme_void() +  # Clean, borderless map
  scale_fill_manual(
    values = c("red", "green", "blue"),
    na.value = "white"  # States with no cluster info shown in white
  ) +
  labs(
    title = "Geographic Distribution of Clusters",
    fill = "Cluster"
  )

```
## Conclusion

This project utilized K-Means clustering and Principal Component Analysis (PCA) on the "Fatalities.csv" dataset from the U.S. Department of Transportation’s Fatal Accident Reporting System (1982–1988) to identify patterns in traffic accident fatalities across U.S. states. The analysis revealed three distinct clusters based on alcohol-related fatalities (afatal), fatalities for ages 15–17 (fatal1517), and ages 18–20 (fatal1820):

-   Cluster 1 (Low-Risk, 112 states): Characterized by low fatalities (81.8 alcohol-related, 19.0 ages 15–17, 31.5 ages 18–20), primarily in rural states like Nebraska and Iowa, where lower traffic density and fewer alcohol-related incidents contribute to safer roads.
-   Cluster 2 (High-Risk, 89 states): Marked by high fatalities (475 alcohol-related, 108 ages 15–17, 175 ages 18–20), concentrated in urban states such as California and Texas, indicating hotspots driven by high traffic volumes and alcohol accessibility.
-   Cluster 3 (Moderate-Risk, 74 states): Exhibited moderate fatalities (256 alcohol-related, 56.3 ages 15–17, 93.8 ages 18–20), representing states with mixed urban-rural characteristics, primarily in the South and West.


PCA analysis showed that the first two principal components explained approximately 80% of the variance, with afatal and fatal1820 as the primary drivers, underscoring the significant role of alcohol and young drivers (ages 18–20) in fatality patterns. Geographic visualization highlighted an urban-rural divide, with high-risk clusters in urbanized states and low-risk clusters in rural areas. The clustering achieved a between_SS/total_SS of 65.2%, indicating a reasonable fit, validated by clear separation in PCA projections and geographic mappings.

The findings reveal a critical narrative of risk disparities across U.S. states, with urban areas (Cluster 2) facing a disproportionate burden of alcohol-related and teen fatalities, particularly among 18–20-year-olds. These insights have significant implications for transportation safety policies:

-   Targeted Enforcement: Urban states like California and Texas require stricter DUI patrols and penalties to address the high incidence of alcohol-related fatalities (475 on average in Cluster 2). Enhanced enforcement could reduce risky behaviors, particularly among young drivers.
-   Education Campaigns: Public awareness programs targeting 18–20-year-olds, especially in urban settings, are essential to curb drinking and driving. College campuses, where this age group is prevalent, could host alcohol education initiatives to promote safer behaviors.
-   Infrastructure Improvements: High-risk urban zones need infrastructure enhancements, such as improved lighting, road signage, or traffic calming measures, to mitigate crash severity, especially in areas with high traffic volumes.

The urban-rural divide suggests that resource allocation should prioritize urban hotspots while maintaining lighter interventions in low-risk rural states (Cluster 1). The moderate-risk Cluster 3 indicates a need for balanced strategies in mixed regions. However, the dataset’s age (1982–1988) limits its applicability to modern contexts, as traffic patterns, vehicle safety, and alcohol regulations have evolved. 

Additionally, K-Means clustering’s sensitivity to centroid initialization necessitates robust validation methods, such as the Elbow and Silhouette approaches used here. Future analyses could incorporate recent data or additional variables (e.g., road conditions, weather, or vehicle types) to enhance hotspot identification, drawing inspiration from successful studies in London and Medellín (ScienceDirect, 2007, 2020). These findings provide a data-driven foundation for policymakers to reduce traffic fatalities, particularly in high-risk urban areas, ultimately saving lives through targeted, evidence-based interventions.

## References

-   Ahmed, M., Seraj, R., & Islam, S. M. S. (2020). The K-means algorithm: A comprehensive survey and performance evaluation. Electronics, 9(8), 1295.

-   Anchalia, P. P., Koundinya, A. K., & Srinath, N. K. (2013). MapReduce design of K-means clustering algorithm. In 2013 International Conference on Information Science and Applications (ICISA) (pp. 1–5). IEEE.

-   Bao, C. (2021). K-means clustering algorithm: A brief review. Francis Academic Press.

-   Chougule, P., Thakare, A. D., Kale, P., Gole, M., & Nanekar, P. (2015). Genetic K-means clustering algorithm for documents. International Journal of Computer Applications, 6(2), 1724–1727.

-   Dubey, S. R., Dixit, P., Singh, N., & Gupta, J. P. (2013). Infected fruit part detection using K-means clustering segmentation technique. International Journal of Interactive Multimedia and Artificial Intelligence.

-   Fahlevi, M. R., Putri, D. R. D., Putri, F. A., Rahman, M., Sipahutar, L., & Muhatri, M. (2020). Determination of rice quality using the K-means clustering method. In 2020 2nd International Conference on Cybernetics and Intelligent System (ICORIS) (pp. 1–6).

-   Javadi, S., Hashemy, S. M., Mohammadi, K., Howard, K. W. F., & Neshat, A. (2017). Classification of aquifer vulnerability using K-means cluster analysis. Journal of Hydrology, 549, 27–37.

-   Kanungo, T., Mount, D. M., Netanyahu, N. S., Piatko, C. D., Silverman, R., & Wu, A. Y. (2002). An efficient K-means clustering algorithm: Analysis and implementation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(7), 881–892.

-   Kumar, V., Chhabra, J. K., & Kumar, D. (2014). Automatic cluster evolution using gravitational search algorithm and its application on image segmentation. Engineering Applications of Artificial Intelligence, 29, 93–103.

-   Lai, D., Zhang, Y., Zhang, X., Su, Y., & Bin, M. A. (2024). Silhouette-coefficient based weighting K-means. Neural Computing and Applications.

-   Na, S., Xumin, L., & Yong, G. (2010). Research on K-means clustering algorithm: An improved K-means clustering algorithm. In 2010 Third International Symposium on Intelligent Information Technology and Security Informatics (pp. 63–67). IEEE.

-   Qabbaah, H., Sammour, G., & Vanhoof, K. (2019). Using K-means clustering and data visualization for monetizing logistics data. In 2019 2nd International Conference on New Trends in Computing Sciences (ICTCS) (pp. 1–6). IEEE.

-   Raykov, Y. P., Boukouvalas, A., Baig, F., & Little, M. A. (2016). What to do when K-means clustering fails: A simple yet principled alternative algorithm. PloS One, 11(9), e0162259.

-   Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20, 53–65.

-   Semoglou, A., Likas, A., & Pavlopoulos, G. A. (2025). Silhouette-guided instance-weighted K-means (K-Sil). arXiv.

-   Shi, C., Wei, B., Wei, S., Wang, W., Liu, H., & Liu, J. (2021). A quantitative discriminant method of elbow point for the optimal number of clusters in clustering algorithm. EURASIP Journal on Wireless Communications and Networking, 2021, 1–16.

-   Sinaga, K. P., & Yang, M.-S. (2020). Unsupervised K-means clustering algorithm. IEEE Access, 8, 80716–80727.

-   Syakur, M. A., Khotimah, B. K., Rochman, E. M. S., & Satoto, B. D. (2018). Integration K-means clustering method and elbow method for identification of the best customer profile cluster. In IOP Conference Series: Materials Science and Engineering (Vol. 336, p. 012017).

-   Tibshirani, R., Walther, G., & Hastie, T. (2001). Estimating the number of clusters in a data set via the gap statistic. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), 411–423.

-   Ultsch, A., & Lötsch, J. (2022). Euclidean distance-optimized data transformation for cluster analysis in biomedical data. BMC Bioinformatics, 23(1), 233.

-   Usami, S. (2014). Constrained K-means clustering and its application to pattern recognition. The Japanese Journal of Behaviormetrics, 41(1), 25–34.

-   Yuan, C., & Yang, H. (2019). Research on K-value selection method of K-means clustering algorithm. Journal of Multidisciplinary Scientific Journal, 2(2), 226–235.

-   Kumar, S., & Toshniwal, D. (2015). A data mining framework to analyze road accident data. Journal of Big Data, 2(1), 1–22.

-   Sadeghi, A., Farhad, H., Mohammadzadeh Moghaddam, A., & Jalili Qazizadeh, M. (2024). Identifying accident prone areas and factors influencing the severity of crashes using machine learning and spatial analyses. Scientific Reports, 14(1), 15736.

-   Anderson, T. K. (2009). Kernel density estimation and K-means clustering to profile road accident hotspots. Accident Analysis & Prevention, 41(3), 359–364.

-   Ocampo, L., Alvarez, P., & Ospina, J. (2020). Modelling road traffic collisions using clustered zones based on Foursquare data in Medellín. Transportation Research Interdisciplinary Perspectives, 7, 100203.

**GitHub. (2020). Workzone-collision-analysis/capstone. https://github.com/workzone-collision-analysis/capstone**