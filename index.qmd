---
title: "Clustering Traffic Accidents Using K-Means Clustering"
subtitle: "This is a Report"
author: "Naveen Katepalli, Mrinal Mandapaka, Adiba Khan"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
    link-citations: true  # <--- ADD THIS LINE INSIDE HTML FORMAT
course: Capstone Projects in Data Science
bibliography: references.bib
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---


Slides: [slides.html](slides.html){target="_blank"} 

## Introduction


## Project Overview

This capstone project applies K-Means Clustering to analyze traffic accident data, aiming to identify high-risk areas ("hotspots") and uncover contributing factors such as location, time, weather, or road conditions. By grouping similar accidents into clusters, the project provides actionable insights for transportation authorities to implement targeted safety measures, reducing accident frequency and severity (Road accident prediction and model interpretation using a hybrid K-means and random forest algorithm approach, Discover Applied Sciences (2020)).

## K-Means Clustering for Traffic Accident Analysis

K-Means Clustering, an unsupervised learning algorithm, segments traffic accident data into K clusters, ensuring incidents within a cluster share similar characteristics while differing from other clusters [@bao2021kmeans]. The process involves:

-   Centroid Initialization: 
Select K initial centroids, ideally using principal component analysis (PCA) to enhance convergence [@zubair2022efficient].

-   Data Assignment: 
Assign each accident record to the nearest centroid based on Euclidean distance, using features like geographic coordinates, time, or weather [@fahlevi2020implementation].

-   Centroid Update: 
Recalculate centroids as the mean of assigned points [@dubey2013infected].

-   Iteration: 
Repeat until clusters stabilize [@javadi2017classification].
This project uses datasets like New York City’s construction-related accidents or Indian traffic data to cluster incidents by spatial, temporal, or environmental factors, identifying high-risk zones [@workzone2020].

  Determining the Optimal Number of Clusters
  Selecting the optimal K ensures meaningful accident clusters. Methods include:

-   Elbow Method: Plots sum of squared errors (SSE) against K, with the "elbow" indicating the optimal number of clusters [@syakur2018integration].

-   Silhouette Method: Evaluates cluster quality using silhouette coefficients (-1 to 1), where 1 indicates well-separated clusters, 0 suggests overlap, and -1 implies misassignment [@rousseeuw1987silhouettes]. Weighted variants improve accuracy [@lai2024silhouette].

-   Gap Statistic: Compares within-cluster dispersion to a reference distribution [@tibshirani2001estimating].
The project employs the Elbow and Silhouette methods to determine K, ensuring robust hotspot identification [@wikipedia2025kmeans], [@wikipedia2025silhouette].

### Optimization and Preprocessing

-   Large accident datasets benefit from the Canopy algorithm, which partitions data into subsets for efficient clustering [@yuan2019research]. 
-   PCA reduces dimensionality, focusing on key features like location or road conditions [@zubair2022efficient].
-   Silhouette-weighted K-Means enhances cluster quality by prioritizing well-separated incidents [@lai2024silhouette].

### Cluster Quality Evaluation

-   The Calinski-Harabasz index validates cluster quality, with higher values indicating well-defined accident clusters [@qabbaah2019clustering]

### Application to Traffic Accident Hotspots

The project clusters traffic accident data to identify hotspots and contributing factors. For example, a London study used K-Means with kernel density estimation to group accidents into five categories and 15 clusters based on environmental factors, guiding safety campaigns [@anderson2007kernel]. In India, K-Modes Clustering on 11,574 accidents in Dehradun revealed patterns when combined with association rule mining [@kumar2015data]. Similarly, in Medellín, Colombia, K-Means clustered zones by venue composition to analyze collision trends [@ocampo2020modelling]. This project analyzes datasets like New York City’s 20,000+ construction-related accidents to identify high-risk road segments, potentially due to poor lighting or road geometry, as seen in Iranian studies [@sadeghi2024identifying]. Results can guide interventions like improved signage or traffic management.

## Variations of K-Means

Variants enhance K-Means for accident data:

-   Constrained K-Means: Ensures meaningful cluster sizes for hotspot analysis [@usami2014constrained].

-   Improved K-Means: Reduces computational overhead for large datasets [@na2010research].

-   Filtering Algorithm: Uses kd-trees for efficient centroid selection [@kanungo2002efficient].

-   Genetic-Based K-Means (GBKM): Optimizes centroids via genetic algorithms [@chougule2015genetic].
MapReduce K-Means: Processes large-scale accident data [@anchalia2013mapreduce].

-   Silhouette-Weighted K-Means: Improves hotspot detection by weighting incidents [@semoglou2025silhouette].
Limitations

K-Means struggles with random centroid initialization, which may lead to inconsistent hotspots. Predefining K risks variability, and the algorithm falters with complex, heterogeneous accident data. Careful preprocessing, such as handling missing values and standardizing features, is crucial [@ahmed2020kmeans], [@wikipedia2025kmeans].

K-Means struggles with random centroid initialization, which may lead to inconsistent hotspots. Predefining K risks variability, and the algorithm falters with complex, heterogeneous accident data. Careful preprocessing, such as handling missing values and standardizing features, is crucial [@ahmed2020kmeans].

### Project Impact

By identifying accident hotspots and patterns, this project supports transportation authorities in prioritizing interventions, such as enhanced lighting or road redesign, to improve safety [@author2020road]. The findings align with studies like those in London and Medellín, offering data-driven solutions to reduce traffic accidents [@anderson2007kernel].

## Methods
The approach to K-Means Clustering for this capstone project involves preparing the data, selecting relevant variables, determining the ideal number of clusters, executing the clustering process, and analyzing the resulting groups to identify traffic accident patterns.

### Data Preparation

Data preparation begins by removing categorical variables (e.g., vehicle type), eliminating outliers (e.g., incorrect coordinates), and applying feature scaling (e.g., normalizing location or time data) to ensure consistency (Raykov et al. 2016).

### Variable Selection

Key variables are chosen, such as accident coordinates, time, weather, and severity, to highlight factors influencing traffic accident hotspots[@unknown2015data].

### Optimal Cluster Determination

The optimal number of clusters is assessed using three techniques:

-   Silhouette Coefficient: Evaluates how well each accident fits its cluster compared to others, with scores from -1 to 1 (1 indicating a good fit). It is calculated as (Shi et al. 2021):
$$S = \frac{b - a}{\max(a, b)}$$
where a is the average intra-cluster distance, and b is the minimum average distance to other clusters.

-   Elbow Method: Plots within-cluster sum of squares (WCSS) against cluster numbers, selecting the point where the decrease rate slows. WCSS is defined as (Shi et al. 2021): 
$$WCSS = \sum_{k=1}^{K} \sum_{x \in C_k} ||x - \mu_k||^2$$
where x is a data point, $C_k$ is the cluster, $μ_k$ is the centroid, and K is the total clusters.

-   Gap Statistic: Compares within-cluster dispersion to a null reference to find the best K (Tibshirani, Walther & Hastie 2001):
 $$Gap_n(k) = E_n^*\{\log(W_k)\} - \log(W_k)$$
 

 where $E_n$ is the expected dispersion, n is sample size, and $W_k$ is within-cluster sum of squares.

### Clustering Process
K-Means Clustering is an iterative procedure (Fahlevi et al. 2020):

-   Set the number of clusters.

-   Choose initial centroids randomly.

-   Assign data points to the nearest centroid using Euclidean distance: 
$$d(x, y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}$$

where x and y are points (Ultsch & Lötsch 2022).

-   Recalculate centroids as the mean of assigned points.

-   Repeat until centroids stabilize.




## Analysis and Results

### Data Exploration and Visualization
A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(ggplot2)
library(dplyr)
library(cluster)
library(factoextra)
library(tidyr)
library(psych)
library(stats)
library(corrplot)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data

df <- read.csv("Fatalities.csv")  # Use your actual CSV file path

Fatalities_Grouped_By_State <- df %>%
  group_by(state, year) %>%
  summarise(
    afatal = sum(afatal, na.rm = TRUE),
    fatal1517 = sum(fatal1517, na.rm = TRUE),
    fatal1820 = sum(fatal1820, na.rm = TRUE),
    .groups = "drop"
  )

print(Fatalities_Grouped_By_State)
print(summary(Fatalities_Grouped_By_State))
glimpse(Fatalities_Grouped_By_State)

```

```{r, warning=FALSE, echo=TRUE}
hist_plot <- ggplot(Fatalities_Grouped_By_State, aes(x = afatal)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "black") +
  labs(title = "Distribution of Alcohol-Related Fatalities",
       x = "Alcohol Fatalities", y = "Frequency")
print(hist_plot)
```

### Interpretation:

-   Most states have alcohol fatalities clustered in lower ranges (under ~400).

-   A few extreme values (e.g., >1000) suggest outliers—likely populous states like California or Texas.

-   Distribution is right-skewed, meaning a few states experience disproportionately high fatalities.

```{r, warning=FALSE, echo=TRUE}
scatter_1517 <- ggplot(Fatalities_Grouped_By_State, aes(x = afatal, y = fatal1517)) +
  geom_point(color = "darkred") +
  labs(title = "Alcohol Fatalities vs Fatalities Ages 15-17",
       x = "Alcohol Fatalities", y = "Fatalities Ages 15-17")
print(scatter_1517)
```

### Interpretation:

-   A positive trend is visible: states with higher afatal tend to have more teen fatalities.

-   However, there is scatter and noise, suggesting other factors also influence teen fatalities (e.g., policy, enforcement).

```{r, warning=FALSE, echo=TRUE}
scatter_1820 <- ggplot(Fatalities_Grouped_By_State, aes(x = afatal, y = fatal1820)) +
  geom_point(color = "darkgreen") +
  labs(title = "Alcohol Fatalities vs Fatalities Ages 18-20",
       x = "Alcohol Fatalities", y = "Fatalities Ages 18-20")
print(scatter_1820)

```
### Interpretation:
-   Shows a stronger linear relationship than with ages 15–17.

-   Suggests this age group contributes more significantly to overall alcohol fatalities.

-   Could point to risky behavior or access to alcohol in college-age populations.


```{r correlation-plot, echo=TRUE, warning=FALSE, message=FALSE}
# Load required libraries
library(dplyr)
library(corrplot)

# Select numeric columns
numeric_cols <- Fatalities_Grouped_By_State %>%
  select(afatal, fatal1517, fatal1820)

# Compute correlation matrix
cor_matrix <- cor(numeric_cols, use = "complete.obs")

# Plot the correlation matrix
corrplot(cor_matrix, method = "number", type = "upper")

```

### Interpretation:
-   afatal vs fatal1820 = 0.8 → strong positive correlation

-   afatal vs fatal1517 = 0.6 → moderate correlation

-   fatal1517 vs fatal1820 = 0.7 → teens in both groups follow similar trends


## Modeling and Results

This section details the data preprocessing, modeling approach, and key findings, weaving a narrative about how the data reveals patterns in traffic accident fatalities across U.S. states from 1982–1988. The story focuses on identifying high-risk areas and understanding the role of alcohol and young drivers in fatal crashes.

### Data Preprocessing and Cleaning
To ensure robust clustering, the "Fatalities.csv" dataset was meticulously preprocessed:

-   Variable Selection: Focused on afatal (alcohol-related fatalities), fatal1517 (fatalities ages 15–17), and fatal1820 (fatalities ages 18–20) to capture alcohol-related and teen-related accident patterns, critical for identifying high-risk hotspots.
-   Missing Data Check: No missing or null values were found, ensuring data integrity.

-   Outlier Removal: Outliers were identified using the interquartile range (IQR) method, removing values beyond ( Q1 - 1.5 \times IQR ) or ( Q3 + 1.5 \times IQR ) to prevent skewing cluster centroids.

-   Feature Scaling: Standardized afatal, fatal1517, and fatal1820 to a mean of 0 and standard deviation of 1, as K-Means clustering relies on Euclidean distance, and unscaled data could bias results.

**Tell a story about what the data reveals.**

```{r}
# Define a function to detect outliers using the IQR method
detect_outliers <- function(x) {
  q1 <- quantile(x, probs = 0.25, na.rm = TRUE)
  q3 <- quantile(x, probs = 0.75, na.rm = TRUE)
  iqr <- q3 - q1
  x < (q1 - 1.5 * iqr) | x > (q3 + 1.5 * iqr)
}

# Define a function to eliminate rows with outliers from specified columns
eliminate_outliers <- function(df, cols = names(df)) {
  for (col in cols) {
    df <- df[!detect_outliers(df[[col]]), ]
  }
  return(df)
}

# Specify the columns to check for outliers
target_columns <- c("afatal", "fatal1517", "fatal1820")

# Remove rows with outliers in the target columns
cleaned_data <- eliminate_outliers(Fatalities_Grouped_By_State, target_columns)

# Scale the cleaned data for clustering or further analysis
scaled_data <- scale(cleaned_data[, target_columns])

# Visualizing Scaled Data
boxplot(
  scaled_data,
  ylab = "Scaled Values",
  main = "Boxplot of Scaled Variables (After Outlier Removal)"
)

```


```{r}
# Determining Optimal Number of Clusters: Elbow Method
library(factoextra)  # For visualizing clustering results

# Apply the Elbow Method using within-cluster sum of squares (WSS)
fviz_nbclust(
  scaled_data,             # The scaled data prepared for clustering
  FUNcluster = kmeans,     # Clustering method: k-means
  method = "wss"           # WSS: within-cluster sum of squares
) +
  labs(
    title = "Elbow Method for Determining Optimal Clusters",
    x = "Number of Clusters (k)",
    y = "Total Within-Cluster Sum of Squares"
  )

```
-     The Elbow Method plot shows the within-cluster sum of squares (WCSS) decreasing as the number of clusters increases, with a noticeable "elbow" at ( K = 3 ).


```{r}
# K-Means Clustering of Traffic Fatalities
set.seed(123)

# Perform K-Means clustering with 3 clusters and 25 random starts
kmeans_result <- kmeans(
  scaled_data,      # Scaled data
  centers = 3,      # Number of clusters
  nstart = 25       # Number of random initial configurations
)

# Add the cluster assignments to the cleaned dataset
cleaned_data$cluster <- as.factor(kmeans_result$cluster)

# Visualizing the Clusters
library(factoextra)

# Visualize the clusters using fviz_cluster
fviz_cluster(
  object = kmeans_result,          # K-means result object
  data = scaled_data,              # Scaled data used for clustering
  ellipse.type = "convex",         # Convex hull around clusters
  geom = "point",                  # Points for each observation
  palette = "jco",                 # Color palette
  ggtheme = theme_minimal()        # Minimal ggplot theme
) +
  labs(
    title = "K-Means Clustering of Traffic Fatalities",
    x = "Principal Component 1",
    y = "Principal Component 2"
  )

```

-     The visualization shows data points grouped into three clusters with convex ellipses, indicating clear separation and validating the choice of ( K = 3 ).

```{r}
library(dplyr)
library(knitr)
library(kableExtra)

# Create cluster summary
cluster_summary <- cleaned_data %>%
  group_by(cluster) %>%
  summarise(
    across(where(is.numeric), ~ mean(.x, na.rm = TRUE)),
    state = NA_character_,
    .groups = "drop"
  )

# Display the summary nicely
cluster_summary %>%
  kable(caption = "Table: Average Values of Numeric Variables by Cluster", digits = 2, booktabs = TRUE) %>%
  kable_styling(full_width = FALSE, position = "center")

```


-   The bar plot visually confirms that Cluster 2 has significantly higher alcohol-related fatalities (475) compared to Cluster 3 (256) and Cluster 1 (81.8).

-   This stark contrast underscores Cluster 2 as the primary target for alcohol-related interventions, as its fatalities are nearly six times higher than Cluster 1’s, highlighting the urgency of addressing urban hotspots.

-   The distinct heights of the bars reinforce the clustering results, showing clear differentiation in risk levels across the three groups.


```{r}
# ---------------------------------------------------------
# Bar Plot: Alcohol-Related Fatalities by Cluster
# ---------------------------------------------------------

# Load required library
library(ggplot2)

# Create bar plot for average alcohol-related fatalities per cluster
ggplot(cluster_summary, aes(x = cluster, y = afatal, fill = cluster)) +
  geom_bar(
    stat = "identity",
    show.legend = FALSE  # Hides redundant legend since color = cluster
  ) +
  scale_fill_manual(values = c("red", "green", "blue")) +
  labs(
    title = "Alcohol-Related Fatalities by Cluster",
    x = "Cluster",
    y = "Average Alcohol-Related Fatalities"
  ) +
  theme_minimal()

```

-   The bar plot visually confirms that Cluster 2 has significantly higher alcohol-related fatalities (475) compared to Cluster 3 (256) and Cluster 1 (81.8).

-   This stark contrast underscores Cluster 2 as the primary target for alcohol-related interventions, as its fatalities are nearly six times higher than Cluster 1’s, highlighting the urgency of addressing urban hotspots.

-   The distinct heights of the bars reinforce the clustering results, showing clear differentiation in risk levels across the three groups.

```{r}
# Principal Component Analysis (PCA)
# Perform PCA on the scaled data
pca_result <- prcomp(
  scaled_data,      # Data already scaled for clustering
  center = TRUE,    # Center the variables
  scale. = TRUE     # Scale the variables to unit variance
)

# Scree Plot: Variance Explained by Principal Components
library(factoextra)

# Visualize the proportion of variance explained by each principal component
fviz_eig(pca_result) +
  labs(
    title = "Scree Plot of PCA Variance Explained",
    x = "Principal Components",
    y = "Percentage of Variance Explained"
  ) +
  theme_minimal()
```

-   The scree plot shows the proportion of variance explained by each principal component, with the first two components accounting for approximately 80% of the total variance.

-   This indicates that afatal, fatal1517, and fatal1820 are effectively summarized by two dimensions, suggesting that alcohol-related and teen fatalities are the primary drivers of differences across states.

-   The sharp drop after the second component confirms that additional components add little explanatory power, validating the use of PCA for dimensionality reduction.


```{r}
# PCA Variable Correlation Circle
# Visualize how original variables contribute to the principal components
fviz_pca_var(
  pca_result,                       # PCA result object
  col.var = "contrib",             # Color by contribution to PCs
  gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),  # Gradient for contribution
  repel = TRUE                     # Avoid label overlap
) +
  labs(
    title = "PCA Variable Correlation Circle",
    x = "Principal Component 1",
    y = "Principal Component 2"
  ) +
  theme_minimal()
```

-   The correlation circle illustrates the relationships between afatal, fatal1517, and fatal1820 in the PCA space.

-   afatal and fatal1820 are closely aligned with the first principal component (PC1), indicating they contribute significantly to the primary source of variance, likely due to their strong correlation (0.8).

-   fatal1517 contributes to both PC1 and PC2, suggesting it plays a secondary but notable role in differentiating states, consistent with its moderate correlation with afatal (0.6).

-   The distinct positioning of variables confirms their unique contributions to the data’s structure, supporting the clustering results.


```{r}

# PCA Projection of Observations by Cluster
# Visualize observations in the PCA space, colored by cluster
fviz_pca_ind(
  pca_result,                   # PCA result object
  geom.ind = "point",           # Use points to represent observations
  col.ind = cleaned_data$cluster,  # Color points by cluster assignment
  palette = "jco",              # Use a clean and readable color palette
  addEllipses = TRUE,           # Draw ellipses around clusters
  legend.title = "Cluster"      # Title for legend
) +
  labs(
    title = "PCA Projection of Observations by Cluster",
    x = "Principal Component 1",
    y = "Principal Component 2"
  ) +
  theme_minimal()
```

-   The PCA projection plot shows individual state-year observations projected onto the first two principal components, colored by cluster with ellipses indicating cluster boundaries.

-   Clear separation between clusters validates the K-Means results: Cluster 2 (high-risk) is distinctly separated from Cluster 1 (low-risk), with Cluster 3 (moderate-risk) occupying an intermediate position.

-   This separation confirms that the clusters capture meaningful differences in fatality patterns, with Cluster 2’s position reflecting its high afatal and fatal1820 values, reinforcing the urban hotspot narrative.



```{r}
# ---------------------------------------------------------
# Geographic Distribution of Clusters
# ---------------------------------------------------------

# Create a lookup table to map state abbreviations to full state names (lowercase)
state_lookup <- data.frame(
  abbr = tolower(state.abb),
  full = tolower(state.name)
)

# Add special handling for Washington, D.C., which isn't included in base R state vectors
state_lookup <- rbind(
  state_lookup,
  data.frame(abbr = "dc", full = "district of columbia")
)

# Convert state abbreviations in the dataset to full state names
cleaned_data$state <- state_lookup$full[match(tolower(cleaned_data$state), state_lookup$abbr)]

# Prepare Data for Mapping -  Get U.S. map data from ggplot2
us_map <- map_data("state")

# Assign cluster values to map data based on matching state names
us_map$cluster <- cleaned_data$cluster[match(us_map$region, cleaned_data$state)]
us_map$cluster <- as.factor(us_map$cluster)

# Plotting the U.S. Map Colored by Cluster
library(ggplot2)
library(maps)

# Plot the map with states filled by cluster color
ggplot(us_map, aes(x = long, y = lat, group = group, fill = cluster)) +
  geom_polygon(color = "black", linewidth = 0.25) +
  theme_void() +  # Clean, borderless map
  scale_fill_manual(
    values = c("red", "green", "blue"),
    na.value = "white"  # States with no cluster info shown in white
  ) +
  labs(
    title = "Geographic Distribution of Clusters",
    fill = "Cluster"
  )

```
-   The geographic map visualizes clusters across U.S. states, with each state colored by its cluster assignment (red for Cluster 1, green for Cluster 2, blue for Cluster 3).

-   Cluster 1 (Low-Risk): Predominantly in rural Midwest and Great Plains states (e.g., Nebraska, Iowa), where low population density and traffic volume contribute to fewer fatalities.


## Conclusion

This project utilized K-Means clustering and Principal Component Analysis (PCA) on the "Fatalities.csv" dataset from the U.S. Department of Transportation’s Fatal Accident Reporting System (1982–1988) to identify patterns in traffic accident fatalities across U.S. states. The analysis revealed three distinct clusters based on alcohol-related fatalities (afatal), fatalities for ages 15–17 (fatal1517), and ages 18–20 (fatal1820):

-   Cluster 1 (Low-Risk, 112 states): Characterized by low fatalities (81.8 alcohol-related, 19.0 ages 15–17, 31.5 ages 18–20), primarily in rural states like Nebraska and Iowa, where lower traffic density and fewer alcohol-related incidents contribute to safer roads.
-   Cluster 2 (High-Risk, 89 states): Marked by high fatalities (475 alcohol-related, 108 ages 15–17, 175 ages 18–20), concentrated in urban states such as California and Texas, indicating hotspots driven by high traffic volumes and alcohol accessibility.
-   Cluster 3 (Moderate-Risk, 74 states): Exhibited moderate fatalities (256 alcohol-related, 56.3 ages 15–17, 93.8 ages 18–20), representing states with mixed urban-rural characteristics, primarily in the South and West.


PCA analysis showed that the first two principal components explained approximately 80% of the variance, with afatal and fatal1820 as the primary drivers, underscoring the significant role of alcohol and young drivers (ages 18–20) in fatality patterns. Geographic visualization highlighted an urban-rural divide, with high-risk clusters in urbanized states and low-risk clusters in rural areas. The clustering achieved a between_SS/total_SS of 65.2%, indicating a reasonable fit, validated by clear separation in PCA projections and geographic mappings.

The findings reveal a critical narrative of risk disparities across U.S. states, with urban areas (Cluster 2) facing a disproportionate burden of alcohol-related and teen fatalities, particularly among 18–20-year-olds. These insights have significant implications for transportation safety policies:

-   Targeted Enforcement: Urban states like California and Texas require stricter DUI patrols and penalties to address the high incidence of alcohol-related fatalities (475 on average in Cluster 2). Enhanced enforcement could reduce risky behaviors, particularly among young drivers.
-   Education Campaigns: Public awareness programs targeting 18–20-year-olds, especially in urban settings, are essential to curb drinking and driving. College campuses, where this age group is prevalent, could host alcohol education initiatives to promote safer behaviors.
-   Infrastructure Improvements: High-risk urban zones need infrastructure enhancements, such as improved lighting, road signage, or traffic calming measures, to mitigate crash severity, especially in areas with high traffic volumes.

The urban-rural divide suggests that resource allocation should prioritize urban hotspots while maintaining lighter interventions in low-risk rural states (Cluster 1). The moderate-risk Cluster 3 indicates a need for balanced strategies in mixed regions. However, the dataset’s age (1982–1988) limits its applicability to modern contexts, as traffic patterns, vehicle safety, and alcohol regulations have evolved. 

Additionally, K-Means clustering’s sensitivity to centroid initialization necessitates robust validation methods, such as the Elbow and Silhouette approaches used here. Future analyses could incorporate recent data or additional variables (e.g., road conditions, weather, or vehicle types) to enhance hotspot identification, drawing inspiration from successful studies in London and Medellín (ScienceDirect, 2007, 2020). These findings provide a data-driven foundation for policymakers to reduce traffic fatalities, particularly in high-risk urban areas, ultimately saving lives through targeted, evidence-based interventions.

## References

::: {#refs}
:::

