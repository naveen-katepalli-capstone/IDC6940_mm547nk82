---
title: "Clustering Traffic Accidents Using K-Means Clustering"
subtitle: "This is a Report Template Quarto"
author: "Naveen Katepalli,
        Mrinal Mandapaka,
        Adiba Khan"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

::: callout-important
**Remember:** Your goal is to make your audience understand and care
about your findings. By crafting a compelling story, you can effectively
communicate the value of your data science project.

Carefully read this template since it has instructions and tips to
writing!
:::

## Introduction


## Project Overview

This capstone project applies K-Means Clustering to analyze traffic accident data, aiming to identify high-risk areas ("hotspots") and uncover contributing factors such as location, time, weather, or road conditions. By grouping similar accidents into clusters, the project provides actionable insights for transportation authorities to implement targeted safety measures, reducing accident frequency and severity (Road accident prediction and model interpretation using a hybrid K-means and random forest algorithm approach, Discover Applied Sciences (2020)).

## K-Means Clustering for Traffic Accident Analysis

K-Means Clustering, an unsupervised learning algorithm, segments traffic accident data into K clusters, ensuring incidents within a cluster share similar characteristics while differing from other clusters (Unsupervised K-Means Clustering Algorithm, Sinaga & Yang, IEEE Access (2020); K-Means Clustering Algorithm: A Brief Review, Bao Chong, Francis Academic Press (2021)). The process involves:

-   Centroid Initialization: 
Select K initial centroids, ideally using principal component analysis (PCA) to enhance convergence (Efficient Feature Selection with Hybrid K-Means Genetic Algorithm for Text Clustering, Zubair et al., 2022 4th International Conference on Electrical, Computer & Telecommunication Engineering (ICECTE) (2022)).

-   Data Assignment: 
Assign each accident record to the nearest centroid based on Euclidean distance, using features like geographic coordinates, time, or weather (Implementation of K-Means Clustering for Scheduling Courses of Lecturers, Fahlevi et al., IOP Conference Series: Materials Science and Engineering (2020)).

-   Centroid Update: 
Recalculate centroids as the mean of assigned points (Infected Fruit Part Detection Using K-Means Clustering Segmentation Technique, Dubey et al., International Journal of Interactive Multimedia and Artificial Intelligence (2013)).

-   Iteration: 
Repeat until clusters stabilize (Classification of Aquifer Vulnerability Using K-Means Cluster Analysis, Javadi et al., Journal of Hydrology (2017)).
This project uses datasets like New York City’s construction-related accidents or Indian traffic data to cluster incidents by spatial, temporal, or environmental factors, identifying high-risk zones (A data mining framework to analyze road accident data, Journal of Big Data (2015); workzone-collision-analysis/capstone, GitHub (2020)).

  Determining the Optimal Number of Clusters
  Selecting the optimal K ensures meaningful accident clusters. Methods include:

-   Elbow Method: Plots sum of squared errors (SSE) against K, with the "elbow" indicating the optimal number of clusters (Integration K-Means Clustering Method and Elbow Method for Identification of The Best Customer Profile Cluster, Syakur et al., IOP Conference Series: Materials Science and Engineering (2018)).

-   Silhouette Method: Evaluates cluster quality using silhouette coefficients (-1 to 1), where 1 indicates well-separated clusters, 0 suggests overlap, and -1 implies misassignment (Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis, Rousseeuw, Journal of Computational and Applied Mathematics (1987); A Quantitative Discriminant Method of Elbow Point for the Optimal Number of Clusters in Clustering Algorithm, Shi et al., EURASIP Journal on Wireless Communications and Networking (2021)). Weighted variants improve accuracy (Silhouette-Coefficient Based Weighting K-Means, Lai et al., Neural Computing and Applications (2024); Silhouette-Guided Instance-Weighted K-Means (K-Sil), Semoglou, Likas & Pavlopoulos, arXiv (2025)).

-   Gap Statistic: Compares within-cluster dispersion to a reference distribution (Estimating the Number of Clusters in a Data Set via the Gap Statistic, Tibshirani, Walther & Hastie, Journal of the Royal Statistical Society: Series B (Statistical Methodology) (2001)).
The project employs the Elbow and Silhouette methods to determine K, ensuring robust hotspot identification (K-Means Clustering, Wikipedia (2025); Silhouette (Clustering), Wikipedia (2025)).

### Optimization and Preprocessing

Large accident datasets benefit from the Canopy algorithm, which partitions data into subsets for efficient clustering (Research on K-Value Selection Method of K-Means Clustering Algorithm, Yuan & Yang, Journal of Multidisciplinary Scientific Journal (2019); Clustering Analysis of Traffic Accident Dataset using Canopy K Means, IEEE Conference Publication (2020)). PCA reduces dimensionality, focusing on key features like location or road conditions (Efficient Feature Selection with Hybrid K-Means Genetic Algorithm for Text Clustering, Zubair et al., 2022 4th International Conference on Electrical, Computer & Telecommunication Engineering (ICECTE) (2022)). Silhouette-weighted K-Means enhances cluster quality by prioritizing well-separated incidents (Silhouette-Coefficient Based Weighting K-Means, Lai et al., Neural Computing and Applications (2024)).

### Cluster Quality Evaluation

The Calinski-Harabasz index validates cluster quality, with higher values indicating well-defined accident clusters (The Clustering Quality Based on Calinski-Harabasz Index, Qabbaah, Sammour & Vanhoof, European Journal of Scientific Research (2019)).

### Application to Traffic Accident Hotspots

The project clusters traffic accident data to identify hotspots and contributing factors. For example, a London study used K-Means with kernel density estimation to group accidents into five categories and 15 clusters based on environmental factors, guiding safety campaigns (Kernel density estimation and K-means clustering to profile road accident hotspots, ScienceDirect (2007)). In India, K-Modes Clustering on 11,574 accidents in Dehradun revealed patterns when combined with association rule mining (A data mining framework to analyze road accident data, Journal of Big Data (2015)). Similarly, in Medellín, Colombia, K-Means clustered zones by venue composition to analyze collision trends (Modelling road traffic collisions using clustered zones based on Foursquare data in Medellín, ScienceDirect (2020)). This project analyzes datasets like New York City’s 20,000+ construction-related accidents to identify high-risk road segments, potentially due to poor lighting or road geometry, as seen in Iranian studies (Identifying accident prone areas and factors influencing the severity of crashes using machine learning and spatial analyses, Scientific Reports (2024); workzone-collision-analysis/capstone, GitHub (2020)). Results can guide interventions like improved signage or traffic management.

## Variations of K-Means

Variants enhance K-Means for accident data:

-   Constrained K-Means: Ensures meaningful cluster sizes for hotspot analysis (Constrained K-Means Clustering and Its Application to Pattern Recognition, Usami, The Japanese Journal of Behaviormetrics (2014)).

-   Improved K-Means: Reduces computational overhead for large datasets (Research on K-Means Clustering Algorithm: An Improved K-Means Clustering Algorithm, Na, Xumin & Yong, 2010 Third International Symposium on Intelligent Information Technology and Security Informatics (2010)).

-   Filtering Algorithm: Uses kd-trees for efficient centroid selection (An Efficient K-Means Clustering Algorithm: Analysis and Implementation, Kanungo et al., IEEE Transactions on Pattern Analysis and Machine Intelligence (2002)).

-   Genetic-Based K-Means (GBKM): Optimizes centroids via genetic algorithms (Genetic K-Means Clustering Algorithm for Documents, Chougule et al., International Journal of Computer Applications (2015)).
MapReduce K-Means: Processes large-scale accident data (MapReduce Design of K-Means Clustering Algorithm, Anchalia, Koundinya & Srinath, 2013 International Conference on Information Science and Applications (ICISA) (2013)).

-   Silhouette-Weighted K-Means: Improves hotspot detection by weighting incidents (Silhouette-Guided Instance-Weighted K-Means (K-Sil), Semoglou, Likas & Pavlopoulos, arXiv (2025)).
Limitations

K-Means struggles with random centroid initialization, which may lead to inconsistent hotspots. Predefining K risks variability, and the algorithm falters with complex, heterogeneous accident data. Careful preprocessing, such as handling missing values and standardizing features, is crucial (The K-Means Algorithm: A Comprehensive Survey and Performance Evaluation, Ahmed, Seraj & Islam, Electronics (2020); K-Means Clustering, Wikipedia (2025)).

### Project Impact

By identifying accident hotspots and patterns, this project supports transportation authorities in prioritizing interventions, such as enhanced lighting or road redesign, to improve safety (Road accident prediction and model interpretation using a hybrid K-means and random forest algorithm approach, Discover Applied Sciences (2020)). The findings align with studies like those in London and Medellín, offering data-driven solutions to reduce traffic accidents (Kernel density estimation and K-means clustering to profile road accident hotspots, ScienceDirect (2007); Modelling road traffic collisions using clustered zones based on Foursquare data in Medellín, ScienceDirect (2020)).

## Methods
The approach to K-Means Clustering for this capstone project involves preparing the data, selecting relevant variables, determining the ideal number of clusters, executing the clustering process, and analyzing the resulting groups to identify traffic accident patterns.

### Data Preparation

Data preparation begins by removing categorical variables (e.g., vehicle type), eliminating outliers (e.g., incorrect coordinates), and applying feature scaling (e.g., normalizing location or time data) to ensure consistency (Raykov et al. 2016).

### Variable Selection

Key variables are chosen, such as accident coordinates, time, weather, and severity, to highlight factors influencing traffic accident hotspots (A data mining framework to analyze road accident data, Journal of Big Data (2015)).

### Optimal Cluster Determination

The optimal number of clusters is assessed using three techniques:

-   Silhouette Coefficient: Evaluates how well each accident fits its cluster compared to others, with scores from -1 to 1 (1 indicating a good fit). It is calculated as (Shi et al. 2021):
$$S = \frac{b - a}{\max(a, b)}$$
where a is the average intra-cluster distance, and b is the minimum average distance to other clusters.

-   Elbow Method: Plots within-cluster sum of squares (WCSS) against cluster numbers, selecting the point where the decrease rate slows. WCSS is defined as (Shi et al. 2021): 
$$WCSS = \sum_{k=1}^{K} \sum_{x \in C_k} ||x - \mu_k||^2$$
where x is a data point, $C_k$ is the cluster, $μ_k$ is the centroid, and K is the total clusters.

-   Gap Statistic: Compares within-cluster dispersion to a null reference to find the best K (Tibshirani, Walther & Hastie 2001):
 $$Gap_n(k) = E_n^*\{\log(W_k)\} - \log(W_k)$$
 

 where $E_n$ is the expected dispersion, n is sample size, and $W_k$ is within-cluster sum of squares.

### Clustering Process
K-Means Clustering is an iterative procedure (Fahlevi et al. 2020):

-   Set the number of clusters.

-   Choose initial centroids randomly.

-   Assign data points to the nearest centroid using Euclidean distance: 
$$d(x, y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}$$

where x and y are points (Ultsch & Lötsch 2022).

-   Recalculate centroids as the mean of assigned points.

-   Repeat until centroids stabilize.




## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(ggplot2)
library(dplyr)
library(cluster)
library(factoextra)
library(tidyr)
library(psych)
library(stats)
library(corrplot)


```

```{r, warning=FALSE, echo=TRUE}
# Load Data

df <- read.csv("Fatalities.csv")  # Use your actual CSV file path

Fatalities_Grouped_By_State <- df %>%
  group_by(state, year) %>%
  summarise(
    afatal = sum(afatal, na.rm = TRUE),
    fatal1517 = sum(fatal1517, na.rm = TRUE),
    fatal1820 = sum(fatal1820, na.rm = TRUE),
    .groups = "drop"
  )

print(Fatalities_Grouped_By_State)

print(summary(Fatalities_Grouped_By_State))

glimpse(Fatalities_Grouped_By_State)

```

```{r, warning=FALSE, echo=TRUE}
hist_plot <- ggplot(Fatalities_Grouped_By_State, aes(x = afatal)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "black") +
  labs(title = "Distribution of Alcohol-Related Fatalities",
       x = "Alcohol Fatalities", y = "Frequency")
print(hist_plot)
```

### Interpretation:
-   Most states have alcohol fatalities clustered in lower ranges (under ~400).
-   A few extreme values (e.g., >1000) suggest outliers—likely populous states like California or Texas.
-   Distribution is right-skewed, meaning a few states experience disproportionately high fatalities.

```{r, warning=FALSE, echo=TRUE}
scatter_1517 <- ggplot(Fatalities_Grouped_By_State, aes(x = afatal, y = fatal1517)) +
  geom_point(color = "darkred") +
  labs(title = "Alcohol Fatalities vs Fatalities Ages 15-17",
       x = "Alcohol Fatalities", y = "Fatalities Ages 15-17")
print(scatter_1517)
```

### Interpretation:
-   A positive trend is visible: states with higher afatal tend to have more teen fatalities.
-   However, there is scatter and noise, suggesting other factors also influence teen fatalities (e.g., policy, enforcement).

```{r, warning=FALSE, echo=TRUE}
scatter_1820 <- ggplot(Fatalities_Grouped_By_State, aes(x = afatal, y = fatal1820)) +
  geom_point(color = "darkgreen") +
  labs(title = "Alcohol Fatalities vs Fatalities Ages 18-20",
       x = "Alcohol Fatalities", y = "Fatalities Ages 18-20")
print(scatter_1820)

```
### Interpretation:
-   Shows a stronger linear relationship than with ages 15–17.
-   Suggests this age group contributes more significantly to overall alcohol fatalities.
-   Could point to risky behavior or access to alcohol in college-age populations.


```{r correlation-plot, echo=TRUE, warning=FALSE, message=FALSE}
# Load required libraries
library(dplyr)
library(corrplot)

# Select numeric columns
numeric_cols <- Fatalities_Grouped_By_State %>%
  select(afatal, fatal1517, fatal1820)

# Compute correlation matrix
cor_matrix <- cor(numeric_cols, use = "complete.obs")

# Plot the correlation matrix
corrplot(cor_matrix, method = "number", type = "upper")

```

### Interpretation:
If you saw values like:
-   afatal vs fatal1820 = 0.8 → strong positive correlation
-   afatal vs fatal1517 = 0.6 → moderate correlation
-   fatal1517 vs fatal1820 = 0.7 → teens in both groups follow similar trends


### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
